{"id":"a7af3cd6-f448-43b9-8855-af6c40252683","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-2WIT4","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"name":"files","value":"","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"buna","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"Client","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"ChatInput"},"dragging":false,"height":289,"id":"ChatInput-2WIT4","position":{"x":552.2432719628514,"y":69.15662353377638},"positionAbsolute":{"x":552.2432719628514,"y":69.15662353377638},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","id":"ParseData-hQKoJ","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{text}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"ParseData"},"dragging":false,"height":353,"id":"ParseData-hQKoJ","position":{"x":2025.5164728641198,"y":329.32469321540464},"positionAbsolute":{"x":2025.5164728641198,"y":329.32469321540464},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-SafNg","node":{"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"You are StarBot, you are the representative of starnet telecommunications company for customer support, customer questions are based on the following context and you will answer in Romanian:\n\nThe context is this: {context}\n\nAnd this is the message History: {history}\n\nThe users question is this: {question}\n\nContextul este acesta: {context}\n"},"history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"history","display_name":"history","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","history","question"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.19"},"type":"Prompt"},"dragging":false,"height":563,"id":"Prompt-SafNg","position":{"x":2486.0988668404975,"y":496.5120474157301},"positionAbsolute":{"x":2486.0988668404975,"y":496.5120474157301},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-5IxHj","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"frozen":false,"icon":"ChatOutput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"data_template":{"advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"StarBot"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.19"},"type":"ChatOutput"},"dragging":false,"height":289,"id":"ChatOutput-5IxHj","position":{"x":3769.242086248817,"y":585.3403837062634},"positionAbsolute":{"x":3769.242086248817,"y":585.3403837062634},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Split text into chunks based on specified criteria.","display_name":"Split Text","id":"SplitText-cLsop","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"data_inputs","value":"","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to split.","title_case":false,"type":"other","_input_type":"HandleInput"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_overlap","value":200,"display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"Number of characters to overlap between chunks.","title_case":false,"type":"int","_input_type":"IntInput"},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_size","value":1000,"display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum number of characters in each chunk.","title_case":false,"type":"int","_input_type":"IntInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def split_text(self) -> list[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = [_input.to_lc_document() for _input in self.data_inputs if isinstance(_input, Data)]\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separator":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"separator","value":"\n","display_name":"Separator","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The character to split on. Defaults to newline.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Split text into chunks based on specified criteria.","icon":"scissors-line-dashed","base_classes":["Data"],"display_name":"Split Text","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"chunks","display_name":"Chunks","method":"split_text","value":"__UNDEFINED__","cache":true}],"field_order":["data_inputs","chunk_overlap","chunk_size","separator"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"type":"SplitText"},"dragging":false,"height":525,"id":"SplitText-cLsop","position":{"x":1658.6730168951926,"y":1122.2322704345925},"positionAbsolute":{"x":1658.6730168951926,"y":1122.2322704345925},"selected":false,"type":"genericNode","width":384},{"id":"GroqModel-yXWKy","type":"genericNode","position":{"x":3148.2857612678326,"y":417.8221443046184},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name=\"groq_api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\"),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return []\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in (\"groq_api_key\", \"groq_api_base\", \"model_name\"):\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key),\n            streaming=stream,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"trace_as_metadata":true,"options":["llama-3.2-90b-vision-preview","gemma2-9b-it","llama-3.2-90b-text-preview","mixtral-8x7b-32768","gemma-7b-it","llama-3.2-3b-preview","llama-3.1-8b-instant","llama3-groq-70b-8192-tool-use-preview","distil-whisper-large-v3-en","whisper-large-v3-turbo","llava-v1.5-7b-4096-preview","llama-3.2-11b-text-preview","llama3-8b-8192","llama-3.1-70b-versatile","whisper-large-v3","llama3-70b-8192","llama3-groq-8b-8192-tool-use-preview","llama-3.2-1b-preview","llama-3.2-11b-vision-preview","llama-guard-3-8b"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama-3.1-70b-versatile","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":"0.4","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["groq_api_base","groq_api_key","max_tokens","model_name","n","stream","temperature"]}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"GroqModel-yXWKy","description":"Generate text using Groq.","display_name":"Groq"},"selected":false,"width":384,"height":603,"positionAbsolute":{"x":3148.2857612678326,"y":417.8221443046184},"dragging":false},{"id":"Memory-xszDP","type":"genericNode","position":{"x":376.94785548413927,"y":545.5122177231433},"data":{"type":"Memory","node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":"","display_name":"External Memory","advanced":false,"input_types":["BaseChatMessageHistory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain.memory import ConversationBufferMemory\n\nfrom langflow.custom import Component\nfrom langflow.field_typing import BaseChatMemory\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import LCBuiltinChatMemory, get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        chat_memory = self.memory or LCBuiltinChatMemory(flow_id=self.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n_messages","value":100,"display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int","_input_type":"IntInput"},"order":{"trace_as_metadata":true,"options":["Ascending","Descending"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"order","value":"Ascending","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine and User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Filter by sender type.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filter by sender name.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{sender_name}: {text}","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Retrieves stored chat messages from Langflow tables or an external memory.","icon":"message-square-more","base_classes":["BaseChatMemory","Data","Message"],"display_name":"Chat Memory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Messages (Data)","method":"retrieve_messages","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Messages (Text)","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true},{"types":["BaseChatMemory"],"selected":"BaseChatMemory","name":"lc_memory","display_name":"Memory","method":"build_lc_memory","value":"__UNDEFINED__","cache":true}],"field_order":["memory","sender","sender_name","n_messages","session_id","order","template"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Memory-xszDP","description":"Retrieves stored chat messages from Langflow tables or an external memory.","display_name":"Chat Memory"},"selected":false,"width":384,"height":347,"positionAbsolute":{"x":376.94785548413927,"y":545.5122177231433},"dragging":false},{"id":"CohereEmbeddings-RvoYf","type":"genericNode","position":{"x":1635.713466865087,"y":1788.8537215844565},"data":{"type":"CohereEmbeddings","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings.cohere import CohereEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass CohereEmbeddingsComponent(LCModelComponent):\n    display_name = \"Cohere Embeddings\"\n    description = \"Generate embeddings using Cohere models.\"\n    icon = \"Cohere\"\n    name = \"CohereEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name=\"cohere_api_key\", display_name=\"Cohere API Key\"),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=True,\n            options=[\n                \"embed-english-v2.0\",\n                \"embed-multilingual-v2.0\",\n                \"embed-english-light-v2.0\",\n                \"embed-multilingual-light-v2.0\",\n            ],\n            value=\"embed-english-v2.0\",\n        ),\n        MessageTextInput(name=\"truncate\", display_name=\"Truncate\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        MessageTextInput(name=\"user_agent\", display_name=\"User Agent\", advanced=True, value=\"langchain\"),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return CohereEmbeddings(\n            cohere_api_key=self.cohere_api_key,\n            model=self.model,\n            truncate=self.truncate,\n            max_retries=self.max_retries,\n            user_agent=self.user_agent,\n            request_timeout=self.request_timeout or None,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"cohere_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"cohere_api_key","value":"","display_name":"Cohere API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"max_retries":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_retries","value":3,"display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"model":{"trace_as_metadata":true,"options":["embed-english-v2.0","embed-multilingual-v2.0","embed-english-light-v2.0","embed-multilingual-light-v2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"embed-english-v2.0","display_name":"Model","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"request_timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"request_timeout","value":"","display_name":"Request Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"truncate":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"truncate","value":"","display_name":"Truncate","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user_agent":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_agent","value":"langchain","display_name":"User Agent","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Generate embeddings using Cohere models.","icon":"Cohere","base_classes":["Embeddings"],"display_name":"Cohere Embeddings","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["cohere_api_key","model","truncate","max_retries","user_agent","request_timeout"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"CohereEmbeddings-RvoYf","description":"Generate embeddings using Cohere models.","display_name":"Cohere Embeddings"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":1635.713466865087,"y":1788.8537215844565},"dragging":false},{"id":"CohereEmbeddings-0aAGO","type":"genericNode","position":{"x":686.3396414104698,"y":943.797508406551},"data":{"type":"CohereEmbeddings","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings.cohere import CohereEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass CohereEmbeddingsComponent(LCModelComponent):\n    display_name = \"Cohere Embeddings\"\n    description = \"Generate embeddings using Cohere models.\"\n    icon = \"Cohere\"\n    name = \"CohereEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name=\"cohere_api_key\", display_name=\"Cohere API Key\"),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=True,\n            options=[\n                \"embed-english-v2.0\",\n                \"embed-multilingual-v2.0\",\n                \"embed-english-light-v2.0\",\n                \"embed-multilingual-light-v2.0\",\n            ],\n            value=\"embed-english-v2.0\",\n        ),\n        MessageTextInput(name=\"truncate\", display_name=\"Truncate\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        MessageTextInput(name=\"user_agent\", display_name=\"User Agent\", advanced=True, value=\"langchain\"),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return CohereEmbeddings(\n            cohere_api_key=self.cohere_api_key,\n            model=self.model,\n            truncate=self.truncate,\n            max_retries=self.max_retries,\n            user_agent=self.user_agent,\n            request_timeout=self.request_timeout or None,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"cohere_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"cohere_api_key","value":"","display_name":"Cohere API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"max_retries":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_retries","value":3,"display_name":"Max Retries","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"model":{"trace_as_metadata":true,"options":["embed-english-v2.0","embed-multilingual-v2.0","embed-english-light-v2.0","embed-multilingual-light-v2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"embed-english-v2.0","display_name":"Model","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"request_timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"request_timeout","value":"","display_name":"Request Timeout","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"truncate":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"truncate","value":"","display_name":"Truncate","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user_agent":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_agent","value":"langchain","display_name":"User Agent","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Generate embeddings using Cohere models.","icon":"Cohere","base_classes":["Embeddings"],"display_name":"Cohere Embeddings","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["cohere_api_key","model","truncate","max_retries","user_agent","request_timeout"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"CohereEmbeddings-0aAGO","description":"Generate embeddings using Cohere models.","display_name":"Cohere Embeddings"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":686.3396414104698,"y":943.797508406551},"dragging":false},{"id":"Chroma-ziN64","type":"genericNode","position":{"x":2233.784169219677,"y":1238.469896155366},"data":{"type":"Chroma","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_duplicates":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_duplicates","value":false,"display_name":"Allow Duplicates","advanced":true,"dynamic":false,"info":"If false, will not add documents that are already in the Vector Store.","title_case":false,"type":"bool","_input_type":"BoolInput"},"chroma_server_cors_allow_origins":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_cors_allow_origins","value":"","display_name":"Server CORS Allow Origins","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_grpc_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_grpc_port","value":"","display_name":"Server gRPC Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_host":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_host","value":"","display_name":"Server Host","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_http_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_http_port","value":"","display_name":"Server HTTP Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_ssl_enabled":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_ssl_enabled","value":false,"display_name":"Server SSL Enabled","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Chroma Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/integrations/vectorstores/chroma\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            value=\"langflow\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        StrInput(\n            name=\"chroma_server_cors_allow_origins\",\n            display_name=\"Server CORS Allow Origins\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"chroma_server_host\",\n            display_name=\"Server Host\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_http_port\",\n            display_name=\"Server HTTP Port\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_grpc_port\",\n            display_name=\"Server gRPC Port\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"chroma_server_ssl_enabled\",\n            display_name=\"Server SSL Enabled\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_duplicates\",\n            display_name=\"Allow Duplicates\",\n            advanced=True,\n            info=\"If false, will not add documents that are already in the Vector Store.\",\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name=\"limit\",\n            display_name=\"Limit\",\n            advanced=True,\n            info=\"Limit the number of records to compare when Allow Duplicates is False.\",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        \"\"\"\n        Builds the Chroma object.\n        \"\"\"\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = \"Could not import Chroma integration package. Please install it with `pip install langchain-chroma`.\"\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: \"Chroma\") -> None:\n        \"\"\"\n        Adds documents to the Vector Store.\n        \"\"\"\n        if not self.ingest_data:\n            self.status = \"\"\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = \"Vector Store Inputs must be Data objects.\"\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"collection_name","value":"Collections","display_name":"Collection Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"limit":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"limit","value":"","display_name":"Limit","advanced":true,"dynamic":false,"info":"Limit the number of records to compare when Allow Duplicates is False.","title_case":false,"type":"int","_input_type":"IntInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":10,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_type":{"trace_as_metadata":true,"options":["Similarity","MMR"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"search_type","value":"Similarity","display_name":"Search Type","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Chroma Vector Store with search capabilities","icon":"Chroma","base_classes":["Data","Retriever","VectorStore"],"display_name":"Chroma DB","documentation":"https://python.langchain.com/docs/integrations/vectorstores/chroma","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"required_inputs":["number_of_results","search_query","search_type"]},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["collection_name","persist_directory","search_query","ingest_data","embedding","chroma_server_cors_allow_origins","chroma_server_host","chroma_server_http_port","chroma_server_grpc_port","chroma_server_ssl_enabled","allow_duplicates","search_type","number_of_results","limit"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Chroma-ziN64","description":"Chroma Vector Store with search capabilities","display_name":"Chroma DB"},"selected":false,"width":384,"height":637,"positionAbsolute":{"x":2233.784169219677,"y":1238.469896155366},"dragging":false},{"id":"Chroma-Xx1O0","type":"genericNode","position":{"x":1098.8742730642016,"y":230.84235152896088},"data":{"type":"Chroma","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_duplicates":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_duplicates","value":false,"display_name":"Allow Duplicates","advanced":true,"dynamic":false,"info":"If false, will not add documents that are already in the Vector Store.","title_case":false,"type":"bool","_input_type":"BoolInput"},"chroma_server_cors_allow_origins":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_cors_allow_origins","value":"","display_name":"Server CORS Allow Origins","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_grpc_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_grpc_port","value":"","display_name":"Server gRPC Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_host":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_host","value":"","display_name":"Server Host","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_http_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_http_port","value":"","display_name":"Server HTTP Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_ssl_enabled":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_ssl_enabled","value":false,"display_name":"Server SSL Enabled","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    Chroma Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/integrations/vectorstores/chroma\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            value=\"langflow\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        StrInput(\n            name=\"chroma_server_cors_allow_origins\",\n            display_name=\"Server CORS Allow Origins\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"chroma_server_host\",\n            display_name=\"Server Host\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_http_port\",\n            display_name=\"Server HTTP Port\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_grpc_port\",\n            display_name=\"Server gRPC Port\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"chroma_server_ssl_enabled\",\n            display_name=\"Server SSL Enabled\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_duplicates\",\n            display_name=\"Allow Duplicates\",\n            advanced=True,\n            info=\"If false, will not add documents that are already in the Vector Store.\",\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name=\"limit\",\n            display_name=\"Limit\",\n            advanced=True,\n            info=\"Limit the number of records to compare when Allow Duplicates is False.\",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        \"\"\"\n        Builds the Chroma object.\n        \"\"\"\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = \"Could not import Chroma integration package. Please install it with `pip install langchain-chroma`.\"\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: \"Chroma\") -> None:\n        \"\"\"\n        Adds documents to the Vector Store.\n        \"\"\"\n        if not self.ingest_data:\n            self.status = \"\"\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = \"Vector Store Inputs must be Data objects.\"\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"collection_name","value":"Collections","display_name":"Collection Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"limit":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"limit","value":"","display_name":"Limit","advanced":true,"dynamic":false,"info":"Limit the number of records to compare when Allow Duplicates is False.","title_case":false,"type":"int","_input_type":"IntInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":10,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_type":{"trace_as_metadata":true,"options":["Similarity","MMR"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"search_type","value":"Similarity","display_name":"Search Type","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Chroma Vector Store with search capabilities","icon":"Chroma","base_classes":["Data","Retriever","VectorStore"],"display_name":"Chroma DB","documentation":"https://python.langchain.com/docs/integrations/vectorstores/chroma","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"required_inputs":["number_of_results","search_query","search_type"]},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["collection_name","persist_directory","search_query","ingest_data","embedding","chroma_server_cors_allow_origins","chroma_server_host","chroma_server_http_port","chroma_server_grpc_port","chroma_server_ssl_enabled","allow_duplicates","search_type","number_of_results","limit"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Chroma-Xx1O0","description":"Chroma Vector Store with search capabilities","display_name":"Chroma DB"},"selected":false,"width":384,"height":637,"positionAbsolute":{"x":1098.8742730642016,"y":230.84235152896088},"dragging":false},{"id":"CohereRerank-ktjHJ","type":"genericNode","position":{"x":1583.5007874862263,"y":218.7455815178975},"data":{"type":"CohereRerank","node":{"template":{"_type":"Component","retriever":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"retriever","value":"","display_name":"Retriever","advanced":false,"input_types":["Retriever"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import cast\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_cohere import CohereRerank\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.field_typing import Retriever, VectorStore\nfrom langflow.io import (\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    MessageTextInput,\n    MultilineInput,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\nfrom langflow.template.field.base import Output\n\n\nclass CohereRerankComponent(LCVectorStoreComponent):\n    display_name = \"Cohere Rerank\"\n    description = \"Rerank documents using the Cohere API and a retriever.\"\n    name = \"CohereRerank\"\n    icon = \"Cohere\"\n\n    inputs = [\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            options=[\n                \"rerank-english-v3.0\",\n                \"rerank-multilingual-v3.0\",\n                \"rerank-english-v2.0\",\n                \"rerank-multilingual-v2.0\",\n            ],\n            value=\"rerank-english-v3.0\",\n        ),\n        SecretStrInput(name=\"api_key\", display_name=\"API Key\"),\n        IntInput(name=\"top_n\", display_name=\"Top N\", value=3),\n        MessageTextInput(\n            name=\"user_agent\",\n            display_name=\"User Agent\",\n            value=\"langflow\",\n            advanced=True,\n        ),\n        HandleInput(name=\"retriever\", display_name=\"Retriever\", input_types=[\"Retriever\"]),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Retriever\",\n            name=\"base_retriever\",\n            method=\"build_base_retriever\",\n        ),\n        Output(\n            display_name=\"Search Results\",\n            name=\"search_results\",\n            method=\"search_documents\",\n        ),\n    ]\n\n    def build_base_retriever(self) -> Retriever:  # type: ignore[type-var]\n        cohere_reranker = CohereRerank(\n            cohere_api_key=self.api_key,\n            model=self.model,\n            top_n=self.top_n,\n            user_agent=self.user_agent,\n        )\n        retriever = ContextualCompressionRetriever(base_compressor=cohere_reranker, base_retriever=self.retriever)\n        return cast(Retriever, retriever)\n\n    async def search_documents(self) -> list[Data]:  # type: ignore[override]\n        retriever = self.build_base_retriever()\n        documents = await retriever.ainvoke(self.search_query, config={\"callbacks\": self.get_langchain_callbacks()})\n        data = self.to_data(documents)\n        self.status = data\n        return data\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> VectorStore:\n        msg = \"Cohere Rerank does not support vector stores.\"\n        raise NotImplementedError(msg)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_metadata":true,"options":["rerank-english-v3.0","rerank-multilingual-v3.0","rerank-english-v2.0","rerank-multilingual-v2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"rerank-multilingual-v3.0","display_name":"Model","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"top_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_n","value":3,"display_name":"Top N","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"user_agent":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_agent","value":"langflow","display_name":"User Agent","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Rerank documents using the Cohere API and a retriever.","icon":"Cohere","base_classes":["Data","Retriever"],"display_name":"Cohere Rerank","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true}],"field_order":["search_query","model","api_key","top_n","user_agent","retriever"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"CohereRerank-ktjHJ","description":"Rerank documents using the Cohere API and a retriever.","display_name":"Cohere Rerank"},"selected":false,"width":384,"height":651,"positionAbsolute":{"x":1583.5007874862263,"y":218.7455815178975},"dragging":false},{"id":"Directory-Nr292","type":"genericNode","position":{"x":1181.077378524047,"y":1170.6896613442868},"data":{"type":"Directory","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data, retrieve_file_paths\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\n\nclass DirectoryComponent(Component):\n    display_name = \"Directory\"\n    description = \"Recursively load files from a directory.\"\n    icon = \"folder\"\n    name = \"Directory\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"path\",\n            display_name=\"Path\",\n            info=\"Path to the directory to load files from.\",\n        ),\n        MessageTextInput(\n            name=\"types\",\n            display_name=\"Types\",\n            info=\"File types to load. Leave empty to load all default supported types.\",\n            is_list=True,\n        ),\n        IntInput(\n            name=\"depth\",\n            display_name=\"Depth\",\n            info=\"Depth to search for files.\",\n            value=0,\n        ),\n        IntInput(\n            name=\"max_concurrency\",\n            display_name=\"Max Concurrency\",\n            advanced=True,\n            info=\"Maximum concurrency for loading files.\",\n            value=2,\n        ),\n        BoolInput(\n            name=\"load_hidden\",\n            display_name=\"Load Hidden\",\n            advanced=True,\n            info=\"If true, hidden files will be loaded.\",\n        ),\n        BoolInput(\n            name=\"recursive\",\n            display_name=\"Recursive\",\n            advanced=True,\n            info=\"If true, the search will be recursive.\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, multithreading will be used.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_directory\"),\n    ]\n\n    def load_directory(self) -> list[Data]:\n        path = self.path\n        types = (\n            self.types if self.types and self.types != [\"\"] else TEXT_FILE_TYPES\n        )  # self.types is already a list due to is_list=True\n        depth = self.depth\n        max_concurrency = self.max_concurrency\n        load_hidden = self.load_hidden\n        recursive = self.recursive\n        silent_errors = self.silent_errors\n        use_multithreading = self.use_multithreading\n\n        resolved_path = self.resolve_path(path)\n        file_paths = retrieve_file_paths(resolved_path, load_hidden, recursive, depth, types)\n\n        if types:\n            file_paths = [fp for fp in file_paths if any(fp.endswith(ext) for ext in types)]\n\n        loaded_data = []\n\n        if use_multithreading:\n            loaded_data = parallel_load_data(file_paths, silent_errors, max_concurrency)\n        else:\n            loaded_data = [parse_text_file_to_data(file_path, silent_errors) for file_path in file_paths]\n        loaded_data = list(filter(None, loaded_data))\n        self.status = loaded_data\n        return loaded_data  # type: ignore[return-value]\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"depth":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"depth","value":0,"display_name":"Depth","advanced":false,"dynamic":false,"info":"Depth to search for files.","title_case":false,"type":"int","_input_type":"IntInput"},"load_hidden":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"load_hidden","value":false,"display_name":"Load Hidden","advanced":true,"dynamic":false,"info":"If true, hidden files will be loaded.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_concurrency":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_concurrency","value":0,"display_name":"Max Concurrency","advanced":true,"dynamic":false,"info":"Maximum concurrency for loading files.","title_case":false,"type":"int","_input_type":"IntInput"},"path":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"path","value":"","display_name":"Path","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Path to the directory to load files from.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"recursive":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"recursive","value":false,"display_name":"Recursive","advanced":true,"dynamic":false,"info":"If true, the search will be recursive.","title_case":false,"type":"bool","_input_type":"BoolInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"silent_errors","value":false,"display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"},"types":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"types","value":"","display_name":"Types","advanced":false,"input_types":["Message"],"dynamic":false,"info":"File types to load. Leave empty to load all default supported types.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"use_multithreading":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"use_multithreading","value":false,"display_name":"Use Multithreading","advanced":true,"dynamic":false,"info":"If true, multithreading will be used.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Recursively load files from a directory.","icon":"folder","base_classes":["Data"],"display_name":"Directory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_directory","value":"__UNDEFINED__","cache":true}],"field_order":["path","types","depth","max_concurrency","load_hidden","recursive","silent_errors","use_multithreading"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Directory-Nr292"},"selected":false,"width":384,"height":461,"dragging":false,"positionAbsolute":{"x":1181.077378524047,"y":1170.6896613442868}}],"edges":[{"className":"","data":{"sourceHandle":{"dataType":"ParseData","id":"ParseData-hQKoJ","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"context","id":"Prompt-SafNg","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ParseData-hQKoJ{œdataTypeœ:œParseDataœ,œidœ:œParseData-hQKoJœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-SafNg{œfieldNameœ:œcontextœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","source":"ParseData-hQKoJ","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-hQKoJœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-SafNg","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false},{"className":"","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-2WIT4","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"question","id":"Prompt-SafNg","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ChatInput-2WIT4{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-SafNg{œfieldNameœ:œquestionœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","source":"ChatInput-2WIT4","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-SafNg","targetHandle":"{œfieldNameœ:œquestionœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false},{"source":"Prompt-SafNg","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-SafNgœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"GroqModel-yXWKy","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-yXWKyœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"GroqModel-yXWKy","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-SafNg","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-SafNg{œdataTypeœ:œPromptœ,œidœ:œPrompt-SafNgœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GroqModel-yXWKy{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-yXWKyœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"GroqModel-yXWKy","sourceHandle":"{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-yXWKyœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-5IxHj","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-5IxHjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-5IxHj","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-yXWKy","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-GroqModel-yXWKy{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-yXWKyœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-5IxHj{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-5IxHjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"Memory-xszDP","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-xszDPœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-SafNg","targetHandle":"{œfieldNameœ:œhistoryœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"history","id":"Prompt-SafNg","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-xszDP","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-xszDP{œdataTypeœ:œMemoryœ,œidœ:œMemory-xszDPœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-SafNg{œfieldNameœ:œhistoryœ,œidœ:œPrompt-SafNgœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"SplitText-cLsop","sourceHandle":"{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-cLsopœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}","target":"Chroma-ziN64","targetHandle":"{œfieldNameœ:œingest_dataœ,œidœ:œChroma-ziN64œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"ingest_data","id":"Chroma-ziN64","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SplitText","id":"SplitText-cLsop","name":"chunks","output_types":["Data"]}},"id":"reactflow__edge-SplitText-cLsop{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-cLsopœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-Chroma-ziN64{œfieldNameœ:œingest_dataœ,œidœ:œChroma-ziN64œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"CohereEmbeddings-RvoYf","sourceHandle":"{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-RvoYfœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"Chroma-ziN64","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œChroma-ziN64œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"Chroma-ziN64","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"CohereEmbeddings","id":"CohereEmbeddings-RvoYf","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-CohereEmbeddings-RvoYf{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-RvoYfœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-ziN64{œfieldNameœ:œembeddingœ,œidœ:œChroma-ziN64œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"ChatInput-2WIT4","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Chroma-Xx1O0","targetHandle":"{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"search_query","id":"Chroma-Xx1O0","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-2WIT4","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-2WIT4{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Chroma-Xx1O0{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"CohereEmbeddings-0aAGO","sourceHandle":"{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-0aAGOœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"Chroma-Xx1O0","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"Chroma-Xx1O0","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"CohereEmbeddings","id":"CohereEmbeddings-0aAGO","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-CohereEmbeddings-0aAGO{œdataTypeœ:œCohereEmbeddingsœ,œidœ:œCohereEmbeddings-0aAGOœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-Xx1O0{œfieldNameœ:œembeddingœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"Chroma-ziN64","sourceHandle":"{œdataTypeœ:œChromaœ,œidœ:œChroma-ziN64œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}","target":"Chroma-Xx1O0","targetHandle":"{œfieldNameœ:œingest_dataœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"ingest_data","id":"Chroma-Xx1O0","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Chroma","id":"Chroma-ziN64","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-Chroma-ziN64{œdataTypeœ:œChromaœ,œidœ:œChroma-ziN64œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-Chroma-Xx1O0{œfieldNameœ:œingest_dataœ,œidœ:œChroma-Xx1O0œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"CohereRerank-ktjHJ","sourceHandle":"{œdataTypeœ:œCohereRerankœ,œidœ:œCohereRerank-ktjHJœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-hQKoJ","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-hQKoJœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-hQKoJ","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"CohereRerank","id":"CohereRerank-ktjHJ","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-CohereRerank-ktjHJ{œdataTypeœ:œCohereRerankœ,œidœ:œCohereRerank-ktjHJœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-hQKoJ{œfieldNameœ:œdataœ,œidœ:œParseData-hQKoJœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"Chroma-Xx1O0","sourceHandle":"{œdataTypeœ:œChromaœ,œidœ:œChroma-Xx1O0œ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}","target":"CohereRerank-ktjHJ","targetHandle":"{œfieldNameœ:œretrieverœ,œidœ:œCohereRerank-ktjHJœ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"retriever","id":"CohereRerank-ktjHJ","inputTypes":["Retriever"],"type":"other"},"sourceHandle":{"dataType":"Chroma","id":"Chroma-Xx1O0","name":"base_retriever","output_types":["Retriever"]}},"id":"reactflow__edge-Chroma-Xx1O0{œdataTypeœ:œChromaœ,œidœ:œChroma-Xx1O0œ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}-CohereRerank-ktjHJ{œfieldNameœ:œretrieverœ,œidœ:œCohereRerank-ktjHJœ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}","className":"","animated":false},{"source":"ChatInput-2WIT4","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"CohereRerank-ktjHJ","targetHandle":"{œfieldNameœ:œsearch_queryœ,œidœ:œCohereRerank-ktjHJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"search_query","id":"CohereRerank-ktjHJ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-2WIT4","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-2WIT4{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2WIT4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-CohereRerank-ktjHJ{œfieldNameœ:œsearch_queryœ,œidœ:œCohereRerank-ktjHJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"Directory-Nr292","sourceHandle":"{œdataTypeœ:œDirectoryœ,œidœ:œDirectory-Nr292œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"SplitText-cLsop","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-cLsopœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"SplitText-cLsop","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Directory","id":"Directory-Nr292","name":"data","output_types":["Data"]}},"id":"reactflow__edge-Directory-Nr292{œdataTypeœ:œDirectoryœ,œidœ:œDirectory-Nr292œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-cLsop{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-cLsopœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","animated":false,"className":""}],"viewport":{"x":-134.76954168244595,"y":-21.216681319825284,"zoom":0.5488866212516206}},"description":"","name":"AI_Custumer_Services","last_tested_version":"1.0.19","endpoint_name":null,"is_component":false}